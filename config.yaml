# config.yaml
# 9/29/25

project:
  name: ai_lora_training
  seed: 42
  input_dir: data
  output_dir: outputs

paths:
  raw_csv: data/raw/df_cleaned.csv
  splits_dir: data/processed
  outputs_dir: outputs
  metrics_dir: outputs/metrics
  figures_dir: outputs/figures
  checkpoints_dir: outputs/checkpoints
  stats_dir: outputs/stats

labels:
  - Thyroid_Cancer
  - Colon_Cancer
  - Lung_Cancer

model:
  primary: mistralai/Mistral-7B-Instruct
  dtype: bf16
  device: cuda

lora:
  r: [8, 16]                # ablation: try both ranks
  alpha: 32
  dropout: 0.05
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]

train:
  epochs: 2
  lr: 1e-4
  batch_tokens: 2048        # effective batch size in tokens
  grad_accum: 16
  seq_len: 512
  early_stop_patience: 1
  class_weights: true       # handle mild imbalance

eval:
  temperature: 0.0
  max_new_tokens: 4
  regex_lock_labels: true

metrics:
  - macro_f1
  - accuracy
  - per_class_f1
  - confusion_matrix
  - latency_p50_p95_ms
  - tokens_per_sec
  - trainable_params_mb
  - training_time_wall

seed: 42
