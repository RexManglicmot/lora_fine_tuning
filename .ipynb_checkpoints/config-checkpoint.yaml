# config.yaml
# 9/29/25

project:
  name: ai_lora_training
  input_dir: data
  output_dir: outputs

paths:
  raw_csv: data/processed/df_cleaned.csv
  splits_dir: data/processed
  outputs_dir: outputs
  metrics_dir: outputs/metrics
  figures_dir: outputs/figures
  checkpoints_dir: outputs/checkpoints
  stats_dir: outputs/stats

labels:
  - Colon_Cancer
  - Lung_Cancer

model:
  primary: mistralai/Mistral-7B-Instruct-v0.3 # mistralai/Mistral-7B-Instruct
  dtype: bf16
  device: cuda

lora:
  r: [8, 16]                # ablation: try both ranks
  alpha: 32
  dropout: 0.05
  target_modules: ["q_proj","k_proj","v_proj","o_proj","gate_proj","up_proj","down_proj"]

train:
  epochs: 8 # 5 # was 2
  lr: 0.0001 # 1e-4
  batch_tokens: 2048        # effective batch size in tokens
  grad_accum: 16
  seq_len: 512
  early_stop_patience: 1

seed: 42
